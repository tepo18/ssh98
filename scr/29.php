#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CLASH META INDUSTRIAL ENGINE
CORE 1 / 10
Core Utils + Logger + Base Models
"""

# =========================
# STANDARD IMPORTS
# =========================
import os
import sys
import re
import time
import json
import base64
import socket
import threading
import hashlib
import traceback
from typing import Any, Dict, List, Optional, Tuple, Union
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse, parse_qs, unquote
import subprocess
import yaml
import asyncio

# ===========================
# PATHS & CONSTANTS
# ===========================
BASE_DIR = "/storage/emulated/0/Download/Akbar98"
os.makedirs(BASE_DIR, exist_ok=True)

INPUT_PATH = os.path.join(BASE_DIR, "input.txt")
# ÿß€åÿ¨ÿßÿØ ŸÅÿß€åŸÑ ÿÆÿßŸÑ€å Ÿà ÿ®ÿßÿ≤ ⁄©ÿ±ÿØŸÜ ÿ®ÿß nano
with open(INPUT_PATH, "w", encoding="utf-8") as f:
    f.write("")
subprocess.call(["nano", INPUT_PATH])

# ÿØÿ±€åÿßŸÅÿ™ ŸÖÿ≥€åÿ± ÿÆÿ±Ÿàÿ¨€å ÿßÿ≤ ⁄©ÿßÿ±ÿ®ÿ±
out_folder = input("Enter output folder name in Download: ").strip()
if not out_folder:
    print("Folder name required."); sys.exit(1)
OUT_DIR = os.path.join("/storage/emulated/0/Download", out_folder)
os.makedirs(OUT_DIR, exist_ok=True)

out_name = input("Enter output file name (without extension): ").strip()
if not out_name:
    print("File name required."); sys.exit(1)
OUT_PATH = os.path.join(OUT_DIR, f"{out_name}.yaml")

INPUT_FILE = INPUT_PATH
OUT_FILE = OUT_PATH

# =========================
# GLOBAL CONSTANTS
# =========================
ENGINE_NAME = "ClashMetaIndustrialEngine"
ENGINE_VERSION = "1.0.0-industrial"
ENGINE_AUTHOR = "AutoGenerated"

DEFAULT_ENCODING = "utf-8"
DEFAULT_TIMEOUT = 1.2
DEFAULT_PING_RETRY = 3

MAX_WORKERS_LOW = 8
MAX_WORKERS_MED = 16
MAX_WORKERS_HIGH = 32
MOBILE_SAFE_WORKERS = 12

PING_HISTORY_SIZE = 10
PING_FAIL_PENALTY = 9999
SAFE_WRITE_INTERVAL = 5.0

# =========================
# LOGGER
# =========================
class Logger:
    LEVELS = {"DEBUG": 10, "INFO": 20, "WARN": 30, "ERROR": 40}
    current_level = LEVELS["INFO"]
    lock = threading.Lock()

    @classmethod
    def set_level(cls, level: str):
        if level in cls.LEVELS:
            cls.current_level = cls.LEVELS[level]

    @classmethod
    def _log(cls, level: str, msg: str):
        if cls.LEVELS[level] < cls.current_level:
            return
        with cls.lock:
            ts = time.strftime("%Y-%m-%d %H:%M:%S")
            print(f"[{ts}] [{level}] {msg}", flush=True)

    @classmethod
    def debug(cls, msg: str):
        cls._log("DEBUG", msg)

    @classmethod
    def info(cls, msg: str):
        cls._log("INFO", msg)

    @classmethod
    def warn(cls, msg: str):
        cls._log("WARN", msg)

    @classmethod
    def error(cls, msg: str):
        cls._log("ERROR", msg)

# =========================
# SAFE FILE WRITER
# =========================
class SafeFileWriter:
    def __init__(self, path: str):
        self.path = path
        self.tmp_path = path + ".tmp"
        self.last_write = 0.0
        self.lock = threading.Lock()

    def write(self, content: str):
        now = time.time()
        if now - self.last_write < SAFE_WRITE_INTERVAL:
            return
        with self.lock:
            try:
                with open(self.tmp_path, "w", encoding=DEFAULT_ENCODING) as f:
                    f.write(content)
                os.replace(self.tmp_path, self.path)
                self.last_write = now
                Logger.debug(f"Safe write OK ‚Üí {self.path}")
            except Exception as e:
                Logger.error(f"Safe write failed: {e}")

# =========================
# BASE64 & STRING UTILS
# =========================
def b64_fix(data: str) -> str:
    if not data:
        return ""
    s = data.strip().replace("\n", "").replace(" ", "")
    s = s.replace("-", "+").replace("_", "/")
    pad = len(s) % 4
    if pad:
        s += "=" * (4 - pad)
    return s

def is_base64(s: str) -> bool:
    if not s or len(s) < 8:
        return False
    try:
        base64.b64decode(b64_fix(s))
        return True
    except Exception:
        return False

def safe_int(v: Any, default: int = 0) -> int:
    try:
        return int(v)
    except Exception:
        try:
            return int(float(v))
        except Exception:
            return default

def sanitize_name(s: str) -> str:
    if not s:
        return ""
    return re.sub(r"[^A-Za-z0-9_\- .]", "", str(s)).strip()

def tail_id(s: str, n: int = 6) -> str:
    if not s:
        return ""
    t = re.sub(r"[-]", "", str(s))
    return t[-n:] if len(t) >= n else t

# =========================
# UNIQUE NAME POOL
# =========================
class UniqueNamePool:
    def __init__(self):
        self.used = set()
        self.lock = threading.Lock()

    def get(self, base: str) -> str:
        base = sanitize_name(base) or "Proxy"
        with self.lock:
            name = base
            idx = 2
            while name in self.used:
                name = f"{base} {idx}"
                idx += 1
            self.used.add(name)
            return name

NAME_POOL = UniqueNamePool()

# =========================
# FINGERPRINT ENGINE
# =========================
def fingerprint_proxy(p: Dict[str, Any]) -> str:
    raw = (
        str(p.get("type")) +
        str(p.get("server")) +
        str(p.get("port")) +
        str(p.get("uuid") or p.get("password") or "")
    )
    return hashlib.sha256(raw.encode()).hexdigest()

# =========================
# BASE PROXY NODE
# =========================
class ProxyNode:
    __slots__ = (
        "name","type","server","port",
        "uuid","password","cipher","network",
        "tls","sni","params","udp","ping","alive",
        "history","weight","priority","fingerprint"
    )

    def __init__(self, data: Dict[str, Any]):
        self.type = (data.get("type") or "").lower()
        self.server = data.get("server")
        self.port = safe_int(data.get("port"))
        self.uuid = data.get("uuid")
        self.password = data.get("password")
        self.cipher = data.get("cipher")
        self.network = data.get("network")
        self.tls = bool(data.get("tls", False))
        self.sni = data.get("sni") or data.get("servername")
        self.udp = bool(data.get("udp", True))

        base_name = data.get("name") or f"{self.type}-{self.server}-{self.port}"
        self.name = NAME_POOL.get(base_name)

        self.params = data
        self.ping = None
        self.alive = False
        self.history = deque(maxlen=PING_HISTORY_SIZE)

        self.weight = safe_int(data.get("weight", 100))
        self.priority = safe_int(data.get("priority", 0))

        self.fingerprint = fingerprint_proxy(data)

    def update_ping(self, value: Optional[int]):
        if value is None:
            self.history.append(PING_FAIL_PENALTY)
            self.alive = False
        else:
            self.history.append(value)
            self.alive = True
            self.ping = value

    def avg_ping(self) -> int:
        if not self.history:
            return PING_FAIL_PENALTY
        return int(sum(self.history) / len(self.history))

    def score(self) -> float:
        return self.avg_ping() / max(self.weight, 1) + self.priority * 10

    def to_dict(self) -> Dict[str, Any]:
        d = dict(self.params)
        d["name"] = self.name
        d["ping"] = self.ping
        d["alive"] = self.alive
        return d
        # ============================================================
# CORE 2-3 / 10
# UNIVERSAL INPUT LOADER & PARSER
# ============================================================

# ---------------------------
# Text Normalizer
# ---------------------------
def normalize_text(raw: str) -> str:
    if not raw:
        return ""
    raw = raw.replace("\r\n", "\n").replace("\r", "\n")
    return raw.strip()

# ---------------------------
# JSON & YAML extractors
# ---------------------------
def extract_json_fragments(text: str) -> List[str]:
    fragments = []
    stack = []
    start = None
    for i, ch in enumerate(text):
        if ch == "{":
            if not stack:
                start = i
            stack.append(ch)
        elif ch == "}":
            if stack:
                stack.pop()
                if not stack and start is not None:
                    fragments.append(text[start:i+1])
                    start = None
    return fragments

def extract_yaml_docs(text: str) -> List[dict]:
    docs = []
    try:
        for doc in yaml.safe_load_all(text):
            if isinstance(doc, dict):
                docs.append(doc)
    except Exception:
        pass
    return docs

# ---------------------------
# Base64 bundle decoder
# ---------------------------
def decode_base64_bundle(line: str) -> List[str]:
    try:
        decoded = base64.b64decode(b64_fix(line)).decode(DEFAULT_ENCODING, "ignore")
        return [l.strip() for l in decoded.splitlines() if l.strip()]
    except Exception:
        return []

# ---------------------------
# Line classifier
# ---------------------------
def classify_line(line: str) -> str:
    l = line.lower()
    if l.startswith(("vmess://","vless://","trojan://","ss://","ssr://","hy://","hy2://","hysteria://","hysteria2://","tuic://","wg://","quic://","grpc://","h2://")):
        return "proxy_url"
    if is_base64(line):
        return "base64"
    if line.startswith("{") and line.endswith("}"):
        return "json"
    if ":" in line and "\n" in line:
        return "yaml"
    return "unknown"

# ---------------------------
# Universal Input Loader
# ---------------------------
class UniversalInputLoader:
    def __init__(self, raw_text: str):
        self.raw = normalize_text(raw_text)
        self.lines: List[str] = []
        self.json_objects: List[dict] = []
        self.yaml_docs: List[dict] = []

    def load(self) -> Dict[str, Any]:
        if not self.raw:
            return {}

        Logger.info("UniversalInputLoader: start loading input")

        # JSON fragments
        for frag in extract_json_fragments(self.raw):
            try:
                obj = json.loads(frag)
                if isinstance(obj, dict):
                    self.json_objects.append(obj)
            except Exception:
                continue

        # YAML docs
        self.yaml_docs.extend(extract_yaml_docs(self.raw))

        # Line-based scan
        for line in self.raw.splitlines():
            line = line.strip()
            if not line:
                continue
            kind = classify_line(line)
            if kind == "base64":
                self.lines.extend(decode_base64_bundle(line))
            else:
                self.lines.append(line)

        Logger.info(f"Input loaded ‚Üí {len(self.lines)} lines | {len(self.json_objects)} json | {len(self.yaml_docs)} yaml")

        return {
            "lines": self.lines,
            "json": self.json_objects,
            "yaml": self.yaml_docs
        }

# ---------------------------
# Dispatch helper
# ---------------------------
def dispatch_lines(lines: List[str]) -> List[str]:
    out = []
    for l in lines:
        if not l or l.startswith("//") or l.startswith("#"):
            continue
        out.append(l)
    return out

# ---------------------------
# Base Parser
# ---------------------------
class BaseParser:
    scheme = ""
    @classmethod
    def match(cls, line: str) -> bool:
        return line.lower().startswith(cls.scheme)
    @classmethod
    def parse(cls, line: str) -> Optional[ProxyNode]:
        raise NotImplementedError

# ---------------------------
# Example: Vless Parser
# ---------------------------
class VlessParser(BaseParser):
    scheme = "vless://"
    @classmethod
    def parse(cls, line: str) -> Optional[ProxyNode]:
        try:
            raw = line[len(cls.scheme):]
            frag = ""
            if "#" in raw:
                raw, frag = raw.split("#", 1)
                frag = unquote(frag)
            if "?" in raw:
                main, qs = raw.split("?",1)
            else:
                main, qs = raw, ""
            if "@" not in main:
                return None
            uuid, hostport = main.split("@",1)
            host, port = hostport.split(":",1) if ":" in hostport else (hostport,"443")
            params = dict(parse_qs(qs))
            net = (params.get("type",params.get("network",["tcp"]))[0]).lower()
            sec = (params.get("security",[""])[0]).lower()
            data = {
                "type":"vless",
                "server":host,
                "port":safe_int(port,443),
                "uuid":uuid,
                "encryption":params.get("encryption",["none"])[0],
                "network":net,
                "tls": sec=="tls",
                "udp":True,
                "name": frag or f"vless-{host}-{tail_id(uuid)}"
            }
            if data["tls"]:
                data["sni"] = params.get("sni", [host])[0]
            if net=="ws":
                data["ws-opts"] = {"path": params.get("path",["/"])[0], "headers":{"Host": params.get("host", [""])[0]}}
            if net=="grpc":
                data["grpc-opts"] = {"grpc-service-name": params.get("serviceName",[""])[0]}
            return ProxyNode(data)
        except Exception:
            return None

# ---------------------------
# PARSER REGISTRY
# ---------------------------
PARSER_REGISTRY = [VlessParser]  # ÿ®ÿπÿØÿßŸã ŸáŸÖŸá parsers ÿßÿ∂ÿßŸÅŸá ÿ¥ŸàŸÜÿØ

def parse_line_to_node(line: str) -> Optional[ProxyNode]:
    for parser in PARSER_REGISTRY:
        if parser.match(line):
            return parser.parse(line)
    return None
    # ============================================================
# CORE 5 / 10
# DEDUPLICATION & NORMALIZATION ENGINE
# ============================================================

# ---------------------------
# ProxyNode Deduplication
# ---------------------------
class ProxyDeduper:
    """Multi-step industrial deduplication engine"""

    @staticmethod
    def key(p: Union[dict, ProxyNode]):
        if isinstance(p, ProxyNode):
            return (p.type, p.server, p.port, p.uuid or p.password or "")
        elif isinstance(p, dict):
            return (p.get("type"), p.get("server"), safe_int(p.get("port")), p.get("uuid") or p.get("password") or "")
        return None

    @classmethod
    def dedupe(cls, proxies: List[Union[dict, ProxyNode]]) -> List[Union[dict, ProxyNode]]:
        seen = set()
        final = []
        for p in proxies:
            k = cls.key(p)
            if k in seen:
                continue
            seen.add(k)
            final.append(p)
        return final

# ---------------------------
# Proxy Normalization
# ---------------------------
class ProxyNormalizer:
    """Normalize proxy entries to Clash Meta compatible dict"""

    @staticmethod
    def normalize(p: Union[dict, ProxyNode]) -> dict:
        if isinstance(p, ProxyNode):
            data = p.to_dict()
        else:
            data = dict(p)

        t = data.get("type", "").lower()
        base = {
            "name": data.get("name") or f"{t}-{data.get('server')}-{tail_id(data.get('port'))}",
            "type": t,
            "server": data.get("server"),
            "port": safe_int(data.get("port", 0)),
        }

        # type-specific fields
        if t == "vless":
            base["uuid"] = data.get("uuid")
            base["encryption"] = data.get("encryption", "none")
            if data.get("network"): base["network"] = data["network"]
            if data.get("tls"): base["tls"] = True
            if data.get("ws-opts"): base["ws-opts"] = data["ws-opts"]
            if data.get("grpc-opts"): base["grpc-opts"] = data["grpc-opts"]

        elif t == "vmess":
            base["uuid"] = data.get("uuid")
            base["alterId"] = safe_int(data.get("alterId", 0))
            base["cipher"] = data.get("cipher", "auto")
            if data.get("network"): base["network"] = data["network"]
            if data.get("ws-opts"): base["ws-opts"] = data["ws-opts"]
            if data.get("grpc-opts"): base["grpc-opts"] = data.get("grpc-opts")
            if data.get("tls"): base["tls"] = True

        elif t == "trojan":
            base["password"] = data.get("password")
            if data.get("sni"): base["sni"] = data["sni"]
            if data.get("ws-opts"): base["ws-opts"] = data["ws-opts"]

        elif t in ("ss", "ssr"):
            base["cipher"] = data.get("cipher")
            base["password"] = data.get("password")
            if t == "ssr":
                base["protocol"] = data.get("protocol")
                base["obfs"] = data.get("obfs")
                base["protocol-param"] = data.get("protocol-param", "")
                base["obfs-param"] = data.get("obfs-param", "")

        # universal fields
        for k in ("udp","network","servername","sni","tls"):
            if k in data and k not in base:
                base[k] = data[k]

        return base

# ---------------------------
# Proxy Merger
# ---------------------------
class ProxyMerger:
    """Merge multiple proxy lists with deduplication and normalization"""

    @staticmethod
    def merge(lists: List[List[Union[dict, ProxyNode]]]) -> List[dict]:
        merged: List[Union[dict, ProxyNode]] = []
        for lst in lists:
            if lst:
                merged.extend(lst)

        merged = ProxyDeduper.dedupe(merged)
        merged_dicts = [ProxyNormalizer.normalize(p) for p in merged]

        type_priority = {"vless": 1, "vmess": 2, "trojan": 3, "ss": 4, "ssr": 5}
        merged_dicts.sort(key=lambda x: type_priority.get(x.get("type"), 99))
        return merged_dicts
        # CORE 6 / 10
# PING ENGINE & HEALTH CHECK
# ============================================================

class PingEngine:
    """Industrial multi-threaded TCP ping engine"""

    def __init__(self, timeout=0.7, threads=50, retries=3):
        self.timeout = timeout
        self.threads = threads
        self.retries = retries

    @staticmethod
    def tcp_ping_once(host: str, port: int, timeout=0.7) -> Optional[int]:
        try:
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.settimeout(timeout)
            start = time.time()
            s.connect((host, port))
            s.close()
            return int((time.time() - start) * 1000)
        except Exception:
            return None

    def tcp_ping(self, host: str, port: int) -> Optional[int]:
        results = []
        for _ in range(self.retries):
            val = self.tcp_ping_once(host, port, self.timeout)
            if val is not None:
                results.append(val)
        if not results:
            return None
        results.sort()
        return results[len(results)//2]

    def check_proxies(self, proxies: List[ProxyNode]) -> List[ProxyNode]:
        if not proxies:
            return []
        with ThreadPoolExecutor(max_workers=self.threads) as executor:
            futures = {executor.submit(self.tcp_ping, p.server, p.port): p for p in proxies}
            for f in futures:
                p = futures[f]
                try:
                    ping = f.result()
                    p.update_ping(ping)
                except Exception:
                    p.update_ping(None)
        return proxies

class SmartAutoSwitch:
    """Auto-switch proxy based on ping"""

    def __init__(self, proxies: List[ProxyNode], config: dict, interval: int = 10):
        self.proxies = [p for p in proxies if p.alive]
        self.config = config
        self.interval = interval
        self.active_proxy: Optional[ProxyNode] = self.proxies[0] if self.proxies else None
        self.DEFAULT_PING = 1300
        self._stop = False
        self.lock = threading.Lock()

    def update_groups(self):
        names = [p.name for p in self.proxies]
        for g in self.config.get("proxy-groups", []):
            if g["name"] in ("Global - UrlTest(Tiers) ü•∂", "Global - Fallback(Tiers) üåé", "Global - LoadBalance(ch) ü¶Ñ", "Global - LoadBalance(rr) üçÑ",
                             "Tier 1 üöÄ", "Tier 2 ‚ö°Ô∏è", "Tier 3 üíé"):
                g["proxies"] = names

    def switch_best_proxy(self):
        best = self.active_proxy
        best_ping = best.ping if best else self.DEFAULT_PING
        for p in self.proxies:
            ping = p.ping if p.ping is not None else self.DEFAULT_PING
            if ping < best_ping:
                best = p
                best_ping = ping
        if best and best != self.active_proxy:
            self.active_proxy = best
            print(f"[‚ö° AutoSwitch] Switched ‚Üí {best.name} ({best_ping} ms)")

    def run_loop(self):
        while not self._stop:
            self.switch_best_proxy()
            self.update_groups()
            try:
                with open(OUT_PATH, "w", encoding="utf-8") as f:
                    yaml.safe_dump(self.config, f, allow_unicode=True, sort_keys=False)
            except Exception:
                pass
            time.sleep(self.interval)

    def start(self):
        threading.Thread(target=self.run_loop, daemon=True).start()

    def stop(self):
        self._stop = True

# ============================================================
# CORE 7 / 10
# FULL CONFIG GENERATOR, PROXY GROUPS & RULES ENGINE
# ============================================================

class ConfigGenerator:
    """Industrial Clash Meta config generator with advanced groups and rules"""

    DEFAULT_RULES = [
        "MATCH,Global ¬©Ô∏è"
    ]

    DEFAULT_GROUPS = [
        {"name": "Global ¬©Ô∏è", "type": "select", "proxies": []},
        {"name": "Global - UrlTest(Tiers) ü•∂", "type": "url-test", "url": "http://clients3.google.com/generate_204", "interval": 25, "tolerance": 50, "proxies": []},
        {"name": "Global - Fallback(Tiers) üåé", "type": "fallback", "url": "http://clients3.google.com/generate_204", "interval": 3, "proxies": []},
        {"name": "Global - LoadBalance(ch) ü¶Ñ", "type": "fallback", "url": "http://clients3.google.com/generate_204", "interval": 20, "timeout": 3, "tolerance": 50, "proxies": []},
        {"name": "Global - LoadBalance(rr) üçÑ", "type": "url-test", "url": "http://clients3.google.com/generate_204", "interval": 1895, "tolerance": 50, "proxies": []},
        {"name": "Tier 1 üöÄ", "type": "fallback", "url": "http://clients3.google.com/generate_204", "interval": 300, "proxies": []},
        {"name": "Tier 2 ‚ö°Ô∏è", "type": "fallback", "url": "http://clients3.google.com/generate_204", "interval": 29860, "timeout": 33, "tolerance": 50, "proxies": []},
        {"name": "Tier 3 üíé", "type": "url-test", "url": "http://clients3.google.com/generate_204", "interval": 1000, "tolerance": 50, "proxies": []}
    ]

    def __init__(self, proxies: List[Union[dict, ProxyNode]]):
        self.proxies = proxies
        self.config = {"proxies": [], "proxy-groups": [], "rules": self.DEFAULT_RULES.copy()}

    def generate(self) -> dict:
        # Deduplicate & normalize
        proxies = ProxyMerger.merge([self.proxies])
        self.config["proxies"] = proxies

        # Fill proxy groups
        names = [p["name"] for p in proxies]
        for g in self.DEFAULT_GROUPS:
            if g["name"] in ("Global ¬©Ô∏è", "Global - UrlTest(Tiers) ü•∂", "Global - Fallback(Tiers) üåé",
                             "Global - LoadBalance(ch) ü¶Ñ", "Global - LoadBalance(rr) üçÑ",
                             "Tier 1 üöÄ", "Tier 2 ‚ö°Ô∏è", "Tier 3 üíé"):
                g["proxies"] = names
        self.config["proxy-groups"] = self.DEFAULT_GROUPS.copy()

        return self.config

# ---------------------------
# Rules Engine
# ---------------------------
class RulesEngine:
    """Industrial rules generator for Clash Meta"""

    @staticmethod
    def add_rule(config: dict, rule: str):
        if "rules" not in config:
            config["rules"] = []
        if rule not in config["rules"]:
            config["rules"].append(rule)

    @staticmethod
    def add_rules(config: dict, rules: List[str]):
        for r in rules:
            RulesEngine.add_rule(config, r)

    @staticmethod
    def create_match_group_rule(group_name: str) -> str:
        return f"MATCH,{group_name}"
# =============================
# Example Execution
# =============================
if __name__ == "__main__":
    if not os.path.exists(INPUT_FILE):
        print("‚ùå input.txt not found")
        sys.exit(1)

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        lines = [l.strip() for l in f.read().splitlines() if l.strip()]

    proxies: List[ProxyNode] = []
    for line in lines:
        node = parse_line_to_node(line)
        if node:
            proxies.append(node)

    # Deduplicate
    proxies = ProxyDeduper.dedupe(proxies)
    print(f"üîπ Parsed & deduped: {len(proxies)} proxies")

    # Ping alive proxies
    pinger = PingEngine()
    proxies = pinger.check_proxies(proxies)

    # Keep alive only and sort by ping
    proxies = [p for p in proxies if p.alive]
    proxies.sort(key=lambda x: x.ping if x.ping is not None else 9999)

    # Build full config
    cfg_generator = ConfigGenerator(proxies)
    config = cfg_generator.generate()

    # Save to file
    with open(OUT_FILE, "w", encoding="utf-8") as f:
        yaml.safe_dump(config, f, allow_unicode=True, sort_keys=False)

    print(f"‚úÖ Ultra config generated ‚Üí {OUT_FILE}")
    print("üöÄ Ultra‚ÄëIndustrial Engine Ready")
